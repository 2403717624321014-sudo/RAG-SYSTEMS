ğŸ§  Enterprise Knowledge Assistant using RAG
ğŸ“Œ Project Overview

This project implements an Enterprise Knowledge Assistant using Retrieval-Augmented Generation (RAG).
The system retrieves relevant information from structured text documents and generates context-grounded answers using a Large Language Model (LLM).

Instead of relying only on the LLMâ€™s internal knowledge, the system injects retrieved document context before generating responses, significantly reducing hallucinations and improving answer reliability.

ğŸ¯ Objectives

The primary objectives of this project are to:

Improve contextual accuracy in AI-generated responses

Reduce hallucinations using document-grounded retrieval

Avoid expensive model fine-tuning

Enable multi-domain question answering

Provide confidence-aware responses

Demonstrate multiple real-world RAG use cases

ğŸ— System Architecture

The system follows a standard RAG pipeline:

Load domain-specific text documents

Split documents into manageable text chunks

Convert chunks into vectors using TF-IDF

Accept a user query

Compute cosine similarity between query and document vectors

Retrieve Top-K relevant chunks

Apply a confidence threshold

Inject retrieved context into the LLM

Generate a grounded response

Display answer with confidence awareness

ğŸ“‚ Use Cases Implemented
1ï¸âƒ£ Internal Employee Knowledge Base (RAG)

Policies, remote work rules, leave policy, IT support, code of conduct

2ï¸âƒ£ Customer Support Ticket Autocomplete (RAG)

Past support tickets, issue resolution, SLA handling

3ï¸âƒ£ Legal & Compliance Document Audit (RAG)

Contracts, compliance clauses, termination terms, data protection

4ï¸âƒ£ General Knowledge Retrieval (RAG)

Landmarks, monuments, engineering achievements (Ondiputhur dataset)

5ï¸âƒ£ Technical Documentation Support (RAG)

API documentation, authentication flows, security guidelines

ğŸ›  Technologies Used

Python

Scikit-learn

TF-IDF Vectorizer

Cosine Similarity

NumPy

Flask (for web interface)

Ollama

LLM Model: Llama3

Retrieval-Augmented Generation (RAG)

ğŸ“ Project Structure
RAG_Use_Cases/
â”‚
â”œâ”€â”€ Emp_Knowledge/
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ index.html
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ employee_data.txt
â”‚   â”œâ”€â”€ README.txt
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ Ticket_Autocomplete/
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ index.html
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ Customer_Ticket_Data.txt
â”‚   â”œâ”€â”€ README.txt
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ Compliance_Audit/
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ index.html
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ legal.txt
â”‚   â”œâ”€â”€ README.txt
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ rag_phi3_textfile_project/
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ index.html
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ ondiputhur.txt
â”‚   â”œâ”€â”€ README.txt
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ Docs_Assist/
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ index.html
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ Tech_data.txt
â”‚   â”œâ”€â”€ README.txt
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ Working_Screens/
â”‚   â”œâ”€â”€ 01_Employee_KB/
â”‚   â”‚   â”œâ”€â”€ 1.png
â”‚   â”‚   â”œâ”€â”€ 2.png
â”‚   â”‚   â””â”€â”€ 3.png
â”‚   â”‚
â”‚   â”œâ”€â”€ 02_Ticket_RAG/
â”‚   â”‚   â”œâ”€â”€ 1.png
â”‚   â”‚   â”œâ”€â”€ 2.png
â”‚   â”‚   â””â”€â”€ 3.png
â”‚   â”‚
â”‚   â”œâ”€â”€ 03_Legal_Audit/
â”‚   â”‚   â”œâ”€â”€ 1.png
â”‚   â”‚   â”œâ”€â”€ 2.png
â”‚   â”‚   â””â”€â”€ 3.png
â”‚   â”‚
â”‚   â”œâ”€â”€ 04_Ondiputhur_RAG/
â”‚   â”‚   â”œâ”€â”€ 1.png
â”‚   â”‚   â”œâ”€â”€ 2.png
â”‚   â”‚   â””â”€â”€ 3.png
â”‚   â”‚
â”‚   â””â”€â”€ 05_Tech_Docs/
â”‚       â”œâ”€â”€ 1.png
â”‚       â”œâ”€â”€ 2.png
â”‚       â””â”€â”€ 3.png
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md


â–¶ How to Run the Project
1ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

2ï¸âƒ£ Ensure Ollama is Running
ollama run llama3

3ï¸âƒ£ Run Any Use Case

Navigate to the required use case folder:

cd Compliance_Audit
python app.py


Open your browser and visit:

http://localhost:5000


Repeat the same steps for other use cases.

ğŸ” Key Features

Domain-specific document retrieval

Top-K chunk retrieval (K = 3)

Confidence-aware answer generation

Context-grounded LLM responses

Reduced hallucination

Modular architecture (one app per use case)

Screenshot-based result validation

ğŸ”¢ Confidence Handling

The system evaluates the similarity score of retrieved chunks:

If similarity â‰¥ threshold â†’ Answer is generated

If similarity < threshold â†’ System avoids unreliable output

This ensures safe and trustworthy responses.

ğŸš€ Future Enhancements

Replace TF-IDF with semantic embeddings

Integrate FAISS for scalable vector storage

Add PDF and DOCX ingestion

Implement conversational memory

Deploy as a unified web application

Add citation highlighting in responses

ğŸ“Œ Conclusion

This project demonstrates how Retrieval-Augmented Generation (RAG) can transform a general-purpose LLM into a reliable, enterprise-ready knowledge assistant.
By grounding responses in authorized documents, the system delivers accurate, explainable, and domain-specific answers, making it ideal for real-world enterprise applications.
